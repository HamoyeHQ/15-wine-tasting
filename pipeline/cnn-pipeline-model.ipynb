{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## 1.0 Getting Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing useful libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Downloading the data from our github repository"},{"metadata":{"trusted":true},"cell_type":"code","source":"url1 = 'https://raw.github.com/HamoyeHQ/stage-f-06-wine-tasting/master/data/top_40_varieties.zip'\nurl2 = 'https://raw.github.com/HamoyeHQ/stage-f-06-wine-tasting/master/data/top_varieties_count.csv'","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_40_varieties = pd.read_csv(url1)\ntop_varieties_count = pd.read_csv(url2)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replacing every occurence of 'US' in country with 'United States of America'\ntop_40_varieties['country'].replace('US', 'United States of America', inplace=True)\n\n# replacing every occurence of 'US' in not_vintage with 'United States of America'\ntop_40_varieties['not_vintage'] = top_40_varieties['not_vintage'].apply(lambda x: x.replace(\\\n                                                                        'US', 'United States of America'))","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# renaming the columns in top_varieties_count\ntop_varieties_count = top_varieties_count.rename(columns={'variety': 'count', 'Unnamed: 0': 'variety'})\ntop_varieties_count = top_varieties_count.set_index('variety') # setting the index\ntop_varieties_count = top_varieties_count['count'] # making it a Series","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top = 20 # selecting top 20 varities as our working varieties. note 1 < n <= 40\n\n# making a datframe of our selecting top n varieties\ntop_df = top_40_varieties[top_40_varieties['variety'].isin(top_varieties_count.iloc[:top].index)]","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# threshold of miniority variety to over sample (use sentences as document instead of the whole description)\nminority_threshold = 5000 \n\n# making a dataframe of the miniority classes\nminority_df = top_df[top_df['variety'].isin(top_varieties_count[top_varieties_count < \\\n                                                                      minority_threshold].index)]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize # importing useful library","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oversampled_miniority_lst = [] # empty list to store sentences as tokens miniority corpus\n\n# creating a function to use sentences as tokens for the miniority classes\ndef over_sample_miniority(row):\n    doc_list = sent_tokenize(row['description'])\n    for sent in doc_list:\n        row['description'] = sent\n        oversampled_miniority_lst.append(list(row))","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"minority_df.apply(over_sample_miniority, axis=1); # over sample the miniority classes","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converts oversampled_miniority_lst to a dataframe\noversampled_miniority_df = pd.DataFrame(oversampled_miniority_lst, columns=minority_df.columns)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# selecting majority classes as a dataframe to concatenate to oversampled_miniority_lst\nmajority_df = top_df[top_df['variety'].isin(\\\n                                        top_varieties_count[top_varieties_count >= minority_threshold].index)]\n\n# concatenates majority_df to oversampled_miniority_lst\nbalanced_df = pd.concat([majority_df, oversampled_miniority_df]) \nbalanced_df = balanced_df.reset_index().drop('index', axis=1) # resets index","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"balanced_variety = balanced_df['variety'].value_counts() # gets a Series of the variety count in balanced_df","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing useful libraries\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for sentence oversampling\nsent_oversample_corpus = [doc1 + ' ' + doc2 for doc1, doc2 in zip(\\\n                                                        balanced_df['description'], balanced_df['not_vintage'])]","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = [label for label in balanced_df['variety']]","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.0 Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a spacy pipeline and disabling tagger, parser and ner to speed up tokenizer\nnlp = spacy.load('en', disable=['tagger', 'parser', 'ner']) ","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = spacy.lang.en.STOP_WORDS # getting spacy's stop-words","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words_lemma = {word.lemma_.lower() for word in nlp(' '.join(stop_words))} | {'-pron-', '10', '12', \n                    'aah', 'aa', 'ab', 'aaa', 'aand', '16', '2', '20', '30', '4', '40', '5', '6', '7', '8', '9'}","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### creating custom transformers to encapsulate our preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GetTokens(BaseEstimator, TransformerMixin):\n    def __init__(self, stop_words=stop_words_lemma):\n        self.stop_words = stop_words\n    \n    # defining tokenzer function to tokenize the lower case lemma of documents in a corpus and \n    # filter out stop-words  \n    def tokenize(self, text):\n        return [word.lemma_.lower() for word in nlp(text) if word.is_alpha and word.lemma_.lower() \\\n                not in self.stop_words]\n\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        self.tokens = [self.tokenize(doc) for doc in X]\n            \n        return self.tokens","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = GetTokens()","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Text2Sequence(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.sequence_tokenizer = Tokenizer(oov_token=-99)\n\n    def fit(self, X, y=None):\n        self.sequence_tokenizer.fit_on_texts(X)\n        self.words_indices = self.sequence_tokenizer.word_index\n        return self\n    \n    def transform(self, X):\n        self.get_sequences = self.sequence_tokenizer.texts_to_sequences(X)\n        return self.get_sequences","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_2_seq = Text2Sequence()","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Padding(BaseEstimator, TransformerMixin):\n    def __init__(self, pad='post'):\n        self.pad = pad\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        self.get_paddings = pad_sequences(X, padding=self.pad)\n        return self.get_paddings","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pad = Padding()","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_prep_pipe = Pipeline([('get_tokens', tokens), ('text_2_sequence', text_2_seq), ('padding', pad)], verbose=1)","execution_count":31,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The above should return preprocessed X_train and y_train from data_prep_pipe.fit_transform(X_train, y_train)"},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec # importing Word2Vec","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_embedding_matrix(model, word_index):\n    vocab_size = len(word_index) + 1\n    embedding_dim = model.wv.vector_size\n    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n    \n    for word in model.wv.vocab:\n        ind = word_index[word]\n        embedding_matrix[ind] = model[word]\n        \n    return embedding_matrix","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import class_weight","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing deep learning libraries\nfrom tensorflow.keras.layers import Embedding, Dense, GlobalMaxPool1D, Conv1D, Dropout\nfrom tensorflow.keras.models import Sequential","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def multi_class_fbeta(ytrue , ypred, beta=1, weighted=True, raw=False, epsilon=1e-7):\n    beta_squared = beta**2\n\n    ytrue = tf.cast(ytrue, tf.float32)\n    ypred= tf.cast(ypred, tf.float32)\n    \n    max_prob = tf.reduce_max(ypred, axis=-1, keepdims=True)\n    ypred = tf.cast(tf.equal(ypred, max_prob), tf.float32)\n        \n    tp = tf.reduce_sum(ytrue*ypred, axis=0)\n    predicted_positive = tf.reduce_sum(ypred, axis=0)\n    actual_positive = tf.reduce_sum(ytrue, axis=0)\n    \n    precision = tp/(predicted_positive+epsilon)\n    recall = tp/(actual_positive+epsilon)\n    \n    fb = (1+beta_squared)*precision*recall / (beta_squared*precision + recall + epsilon)\n    \n    if raw:\n        return fb\n    \n    if weighted:\n        supports = tf.reduce_sum(ytrue, axis=0)\n        return tf.reduce_sum(fb*supports / tf.reduce_sum(supports))\n\n    return tf.reduce_mean(fb)","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_cnn_model(embedding_matrix, input_length):\n    model = Sequential()\n    model.add(Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], \n                           weights=[embedding_matrix], \n                           input_length=input_length, \n                           trainable=False))\n    \n    model.add(Conv1D(128, 3, activation='relu'))\n    model.add(Conv1D(128, 3, activation='relu'))\n    \n    model.add(GlobalMaxPool1D())\n    \n    model.add(Dropout(0.2))\n    \n    model.add(Dense(20, activation='softmax'))\n\n    model.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy', multi_class_fbeta])\n\n    return model","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CNNModel(BaseEstimator, ClassifierMixin):\n    def __init__(self, epochs=5, batch_size=128, verbose=1):\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.class_weight = class_weight\n        self.verbose = verbose\n        \n    def fit(self, X, y):\n        self.corpus = data_prep_pipe.named_steps['get_tokens'].tokens\n        self.w2v_model = Word2Vec(self.corpus, size=300, min_count=1, iter=10)\n        self.embedding_matrix = get_embedding_matrix(self.w2v_model, \\\n                                                     data_prep_pipe.named_steps['text_2_sequence'].words_indices)\n        \n        self.model = build_cnn_model(self.embedding_matrix, X.shape[1])\n        \n        self.le = LabelEncoder()\n        self.one_hot = OneHotEncoder(sparse=False) # initializes a LabelEncoder object\n        \n        encoded_labels = self.le.fit_transform(y)\n        \n        self.class_weights = class_weight.compute_class_weight('balanced', np.arange(20), encoded_labels)\n        self.class_weights = dict(enumerate(self.class_weights))\n\n# encodes the labels\n        \n\n# one_hot encoding the labels\n        one_hot_labels = self.one_hot.fit_transform(encoded_labels.reshape(-1, 1))\n        \n        self.history = self.model.fit(X, one_hot_labels, epochs=self.epochs, batch_size=self.batch_size, \\\n                                      class_weight=self.class_weights, verbose=self.verbose)\n        \n        return self.model\n    \n    def predict(self, X):\n        self.pred = self.model.predict(X)\n        return self.le.inverse_transform(self.one_hot.inverse_transform(self.pred))[0]\n        ","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_model = CNNModel()","execution_count":39,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The above should be fitted on the preprocessed X_train and y_train from data_prep_pipe.fit_transform(X_train, y_train)."},{"metadata":{},"cell_type":"markdown","source":"## Prediction"},{"metadata":{},"cell_type":"markdown","source":"#### the user's input to be predicted will be transformed by being fed into the preprocessor by actually doing somehing like prep_input = data_prep_pipe.transform(user_input) and then be fed into the model by doing something like pred = cnn_model.predict(prep_input)"},{"metadata":{},"cell_type":"markdown","source":"# Note: lines of code below are for demostration purposes and should not be included in our deployment."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Our model only predicts the top 20 varieties and they are as listed below"},{"metadata":{"trusted":true},"cell_type":"code","source":"balanced_df['variety'].unique()","execution_count":40,"outputs":[{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"array(['Riesling', 'Pinot Noir', 'Cabernet Sauvignon', 'Chardonnay',\n       'Red Blend', 'Bordeaux-style Red Blend', 'White Blend',\n       'Portuguese Red', 'Pinot Gris', 'Malbec', 'Merlot',\n       'Sauvignon Blanc', 'Sangiovese', 'Rosé', 'Zinfandel', 'Syrah',\n       'Nebbiolo', 'Rhône-style Red Blend', 'Sparkling Blend',\n       'Tempranillo'], dtype=object)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_pipe = Pipeline([('data_prep', data_prep_pipe), ('model', cnn_model)])","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_pipe.fit(sent_oversample_corpus, labels)","execution_count":42,"outputs":[{"output_type":"stream","text":"[Pipeline] ........ (step 1 of 3) Processing get_tokens, total= 1.1min\n[Pipeline] ... (step 2 of 3) Processing text_2_sequence, total=   5.8s\n[Pipeline] ........... (step 3 of 3) Processing padding, total=   1.9s\nEpoch 1/5\n1248/1248 [==============================] - 10s 8ms/step - loss: 0.8217 - accuracy: 0.7345 - multi_class_fbeta: 0.7350\nEpoch 2/5\n1248/1248 [==============================] - 10s 8ms/step - loss: 0.5963 - accuracy: 0.8005 - multi_class_fbeta: 0.8017\nEpoch 3/5\n1248/1248 [==============================] - 11s 9ms/step - loss: 0.5379 - accuracy: 0.8161 - multi_class_fbeta: 0.8172\nEpoch 4/5\n1248/1248 [==============================] - 10s 8ms/step - loss: 0.5020 - accuracy: 0.8268 - multi_class_fbeta: 0.8277\nEpoch 5/5\n1248/1248 [==============================] - 10s 8ms/step - loss: 0.4719 - accuracy: 0.8344 - multi_class_fbeta: 0.8357\n","name":"stdout"},{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"Pipeline(steps=[('data_prep',\n                 Pipeline(steps=[('get_tokens', GetTokens()),\n                                 ('text_2_sequence', Text2Sequence()),\n                                 ('padding', Padding())],\n                          verbose=1)),\n                ('model', CNNModel())])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Let's do a demo prediction of Bordeaux-style Red Blend from a recent review (published 12/1/2020) not in our training set from [wine ethusiast](https://www.winemag.com/buying-guide/alpha-omega-2017-era-red-napa-valley/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"user_input = \"Juicy and richly interwoven with a fresh underlay of acidity, \" + \\\n             \"this powerhouse blend is substantially etched in cassis, plum and red currant. \" + \\\n             \"The oak and tannin are equally substantial and present, contributing weight, \" + \\\n             \"breadth and length. This will do well to cellar; enjoy best from 2027–2032\"","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Notice the double quotes we used. This is to prevent error should the user's input contain apostrophe."},{"metadata":{"trusted":true},"cell_type":"code","source":"user_input","execution_count":44,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"'Juicy and richly interwoven with a fresh underlay of acidity, this powerhouse blend is substantially etched in cassis, plum and red currant. The oak and tannin are equally substantial and present, contributing weight, breadth and length. This will do well to cellar; enjoy best from 2027–2032'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting user's description\npred = model_pipe.predict([user_input])\nprint(pred)","execution_count":45,"outputs":[{"output_type":"stream","text":"Bordeaux-style Red Blend\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### ...and voila, our model's prediction was as good as right!"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}